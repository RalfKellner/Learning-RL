{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Gridworld-problem with different RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "#solve this maze\n",
    "maze = np.zeros((3, 4))\n",
    "maze[2,0] = 25\n",
    "maze[1,1] = 50\n",
    "maze[0,3] = 75\n",
    "maze[1,3] = 100\n",
    "plt.imshow(maze)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class definition for environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stolen from the lazy programmer:)\n",
    "def print_values(V):\n",
    "    for i in range(3):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(4):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "#stolen from the lazy programmer:)       \n",
    "def print_policy():\n",
    "    for i in range(3):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(4):\n",
    "            try:\n",
    "                pol = agent.possible_actions[(i,j)][np.argmax(agent.action_probs[(i,j)])]\n",
    "            except:\n",
    "                pol = ' '\n",
    "            print(\"  %s  |\" % pol, end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "#class for the gridworld environment\n",
    "class Env():\n",
    "    #set the environment with rewards as we want to\n",
    "    def __init__(self, rewards):\n",
    "        self.i = 2\n",
    "        self.j = 0\n",
    "        self.rewards = rewards\n",
    "     \n",
    "    #set the current position\n",
    "    def set_state(self, state):\n",
    "        self.i, self.j = state\n",
    "    \n",
    "    #go one step with action a, collect the reward, show next state and check if we reached a final state\n",
    "    def step(self, action):\n",
    "        if action == 'L':\n",
    "            self.j -= 1\n",
    "        elif action == 'R':\n",
    "            self.j += 1\n",
    "        elif action == 'D':\n",
    "            self.i += 1\n",
    "        elif action == 'U':\n",
    "            self.i -= 1\n",
    "        \n",
    "        #set next state\n",
    "        next_state = (self.i, self.j)\n",
    "        \n",
    "        #are we in a final state\n",
    "        if (self.i, self.j) == (0,3) or (self.i, self.j) == (1,3):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #the reward of this action\n",
    "        reward = self.rewards[next_state]\n",
    "        \n",
    "        return(reward, next_state, done)\n",
    "        \n",
    "\n",
    "#agent class, defininig all possible actions and corresponding policy which is defined\n",
    "#by probabilities to choose among possible actions\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        #actions the agent can take\n",
    "        self.possible_actions = {\n",
    "            (0,0): ('R', 'D'),\n",
    "            (0,1): ('L', 'R'),\n",
    "            (0,2): ('L', 'R', 'D'),\n",
    "            (1,0): ('D', 'U'),\n",
    "            (1,2): ('D', 'U', 'R'),\n",
    "            (2,0): ('U', 'R'),\n",
    "            (2,1): ('L', 'R'),\n",
    "            (2,2): ('L', 'R', 'U'),\n",
    "            (2,3): ('L', 'U')\n",
    "        }\n",
    "        \n",
    "        #initialize random policy in the beginning\n",
    "        self.action_probs = {\n",
    "            (0,0): [0.5, 0.5],\n",
    "            (0,1): [0.5, 0.5],\n",
    "            (0,2): [1/3, 1/3, 1/3],\n",
    "            (1,0): [0.5, 0.5],\n",
    "            (1,2): [1/3, 1/3, 1/3],\n",
    "            (2,0): [0.5, 0.5],\n",
    "            (2,1): [0.5, 0.5],\n",
    "            (2,2): [1/3, 1/3, 1/3],\n",
    "            (2,3): [0.5, 0.5]\n",
    "        }\n",
    "    \n",
    "    #pi(a | s)\n",
    "    def policy(self, state):\n",
    "        if state in self.possible_actions.keys():\n",
    "            return random.choices(self.possible_actions[state], weights = self.action_probs[state])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rewards for the game to play\n",
    "rewards = {\n",
    "    (0,0): 0,\n",
    "    (0,1): 0,\n",
    "    (0,2): 0,\n",
    "    (0,3): 1,\n",
    "    (1,0): 0,\n",
    "    (1,2): 0,\n",
    "    (1,3): -1,\n",
    "    (2,0): 0,\n",
    "    (2,1): 0,\n",
    "    (2,2): 0,\n",
    "    (2,3): 0\n",
    "}\n",
    "\n",
    "#initialize environment\n",
    "env = Env(rewards)\n",
    "#intialize agent\n",
    "agent = Agent()\n",
    "\n",
    "#to reproduce lazy programmer\n",
    "#agent.action_probs = {\n",
    "#    (0,0): [1., 0.],\n",
    "#    (0,1): [0., 1.],\n",
    "#    (0,2): [0., 1., 0.],\n",
    "#    (1,0): [0., 1.],\n",
    "#    (1,2): [0., 1., 0.],\n",
    "#    (2,0): [1.0, 0.],\n",
    "#    (2,1): [0., 1.],\n",
    "#    (2,2): [0., 0., 1.],\n",
    "#    (2,3): [1., 0.]\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current policy is:\n",
      "---------------------------\n",
      "  R  |  L  |  L  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      " \n",
      "Corresponding values:\n",
      "---------------------------\n",
      " 0.06| 0.15| 0.27| 0.00|\n",
      "---------------------------\n",
      "-0.02| 0.00|-0.37| 0.00|\n",
      "---------------------------\n",
      "-0.11|-0.22|-0.38|-0.67|\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#initialize current value function which asigns a value of 0 to each state\n",
    "V = {}\n",
    "V = V.fromkeys(rewards.keys(), 0)\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "while True:\n",
    "    biggest_delta = 0\n",
    "    #iterate through all states \n",
    "    for s in list(V.keys()):\n",
    "        #if the state is not final, get the action-probability weighted expected value of rewards and future value\n",
    "        if s in agent.possible_actions.keys():\n",
    "            v_new = 0\n",
    "            v_old = V[s]\n",
    "            for a, p in zip(agent.possible_actions[s], agent.action_probs[s]):\n",
    "                env.set_state(s)\n",
    "                reward, next_state, done = env.step(a)\n",
    "                v_new += p * (reward + gamma * V[next_state])\n",
    "            V[s] = v_new\n",
    "            biggest_delta = np.max([biggest_delta, np.abs(v_new - v_old)])\n",
    "        \n",
    "        #if the state is final assign a value of zero\n",
    "        elif s not in agent.possible_actions.keys():\n",
    "            V[s] = 0\n",
    "        \n",
    "    if biggest_delta < 0.001:\n",
    "        break\n",
    "    \n",
    "print('The current policy is:')\n",
    "print_policy()\n",
    "print(' ')\n",
    "print('Corresponding values:')\n",
    "print_values(V)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current policy is:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      " \n",
      "Corresponding values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "#given wew have a value function, choose new policy by being greedy in each state\n",
    "for s in agent.possible_actions.keys():\n",
    "    values_to_evaluate = []\n",
    "    for a in agent.possible_actions[s]:\n",
    "        env.set_state(s)\n",
    "        reward, next_state, done = env.step(a)\n",
    "        values_to_evaluate.append(reward + gamma * V[next_state])\n",
    "\n",
    "    probs = [0.] * len(values_to_evaluate)\n",
    "    probs[np.argmax(values_to_evaluate)] = 1.\n",
    "    agent.action_probs[s] = probs\n",
    "\n",
    "#check is values are higher under the new policy\n",
    "V = {}\n",
    "V = V.fromkeys(rewards.keys(), 0)\n",
    "\n",
    "while True:\n",
    "    biggest_delta = 0\n",
    "    for s in list(V.keys()):\n",
    "        if s in agent.possible_actions.keys():\n",
    "            v_new = 0\n",
    "            v_old = V[s]\n",
    "            for a, p in zip(agent.possible_actions[s], agent.action_probs[s]):\n",
    "                env.set_state(s)\n",
    "                reward, next_state, done = env.step(a)\n",
    "                v_new += p * (reward + gamma * V[next_state])\n",
    "            V[s] = v_new\n",
    "            biggest_delta = np.max([biggest_delta, np.abs(v_new - v_old)])\n",
    "        elif s not in agent.possible_actions.keys():\n",
    "            V[s] = 0\n",
    "        \n",
    "    if biggest_delta < 0.001:\n",
    "        break\n",
    "\n",
    "print('The current policy is:')\n",
    "print_policy()\n",
    "print(' ')\n",
    "print('Corresponding values:')\n",
    "print_values(V)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  L  |  L  |     |\n",
      "---------------------------\n",
      "  D  |     |  D  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  L  |\n"
     ]
    }
   ],
   "source": [
    "rewards = {\n",
    "    (0,0): 0,\n",
    "    (0,1): 0,\n",
    "    (0,2): 0,\n",
    "    (0,3): 1,\n",
    "    (1,0): 0,\n",
    "    (1,2): 0,\n",
    "    (1,3): -1,\n",
    "    (2,0): 0,\n",
    "    (2,1): 0,\n",
    "    (2,2): 0,\n",
    "    (2,3): 0\n",
    "}\n",
    "\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "\n",
    "#to reproduce lazy programmer\n",
    "#agent.action_probs = {\n",
    "#    (0,0): [1., 0.],\n",
    "#    (0,1): [0., 1.],\n",
    "#    (0,2): [0., 1., 0.],\n",
    "#    (1,0): [0., 1.],\n",
    "#    (1,2): [0., 1., 0.],\n",
    "#    (2,0): [1.0, 0.],\n",
    "#    (2,1): [0., 1.],\n",
    "#    (2,2): [0., 0., 1.],\n",
    "#    (2,3): [1., 0.]\n",
    "#}\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final policy is:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      " \n",
      "Corresponding values:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      " \n"
     ]
    }
   ],
   "source": [
    "V = {}\n",
    "V = V.fromkeys(rewards.keys(), 0)\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "#combine the idea of policy evaluation and iteration in one step\n",
    "while True:\n",
    "    biggest_delta = 0\n",
    "    for s in list(V.keys()):\n",
    "        if s in agent.possible_actions.keys():\n",
    "            values_to_evaluate = []\n",
    "            old_v = V[s]\n",
    "            for a in agent.possible_actions[s]:\n",
    "                env.set_state(s)\n",
    "                reward, next_state, done = env.step(a)\n",
    "                values_to_evaluate.append(reward + gamma * V[next_state])\n",
    "\n",
    "            probs = [0.] * len(values_to_evaluate)\n",
    "            probs[np.argmax(values_to_evaluate)] = 1.\n",
    "            agent.action_probs[s] = probs\n",
    "            v_new = np.max(values_to_evaluate)\n",
    "            V[s] = v_new\n",
    "            biggest_delta = np.max([biggest_delta, np.abs(v_new - old_v)])\n",
    "        elif s not in agent.possible_actions.keys():\n",
    "            V[s] = 0\n",
    "            \n",
    "    if biggest_delta < 0.001:\n",
    "        break\n",
    "        \n",
    "print('The final policy is:')\n",
    "print_policy()\n",
    "print(' ')\n",
    "print('Corresponding values:')\n",
    "print_values(V)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monte carlo policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "rewards = {\n",
    "    (0,0): 0,\n",
    "    (0,1): 0,\n",
    "    (0,2): 0,\n",
    "    (0,3): 1,\n",
    "    (1,0): 0,\n",
    "    (1,2): 0,\n",
    "    (1,3): -1,\n",
    "    (2,0): 0,\n",
    "    (2,1): 0,\n",
    "    (2,2): 0,\n",
    "    (2,3): 0\n",
    "}\n",
    "\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "\n",
    "#to reproduce lazy programmer\n",
    "agent.action_probs = {\n",
    "    (0,0): [1., 0.],\n",
    "    (0,1): [0., 1.],\n",
    "    (0,2): [0., 1., 0.],\n",
    "    (1,0): [0., 1.],\n",
    "    (1,2): [0., 1., 0.],\n",
    "    (2,0): [1.0, 0.],\n",
    "    (2,1): [0., 1.],\n",
    "    (2,2): [0., 0., 1.],\n",
    "    (2,3): [1., 0.]\n",
    "}\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n"
     ]
    }
   ],
   "source": [
    "#discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "#initialize state values\n",
    "V = {}\n",
    "V = V.fromkeys(rewards.keys(), 0)\n",
    "\n",
    "#initialize dictionary to collect state returns during simulation\n",
    "returns = {}\n",
    "for s in agent.possible_actions.keys():\n",
    "    returns[s] = []\n",
    "\n",
    "for i in range(100):\n",
    "    #play an episode, collect rewards and calculate returns, given a state\n",
    "\n",
    "    #start exploration method to reach all states\n",
    "    state = random.choice(list(agent.possible_actions.keys()))\n",
    "    env.set_state(state)\n",
    "\n",
    "    #collect states and rewards during episode\n",
    "    states_and_rewards = [(state, 0)]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        reward, state, done = env.step(action)\n",
    "        states_and_rewards.append((state, reward))\n",
    "\n",
    "    #calculate returns, given the state for each episode\n",
    "    G = 0\n",
    "    first = True\n",
    "    states_and_returns = []\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_and_returns.append((s, G))\n",
    "        G = r + gamma * G\n",
    "    states_and_returns.reverse()\n",
    "\n",
    "    #collect the return for the first seen state and update the estimated expected value\n",
    "    seen_states = []\n",
    "    for s, g in states_and_returns:\n",
    "        if s not in seen_states:\n",
    "            returns[s].append(g)\n",
    "            V[s] = np.mean(returns[s])\n",
    "            seen_states.append(s)\n",
    "            \n",
    "print_values(V)\n",
    "\n",
    "\n",
    "#skipping to solve the maze via Monte Carlo Q-learning because the idea stays the same\n",
    "#and we will not use it in this form at a later stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD(0) evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "rewards = {\n",
    "    (0,0): 0,\n",
    "    (0,1): 0,\n",
    "    (0,2): 0,\n",
    "    (0,3): 1,\n",
    "    (1,0): 0,\n",
    "    (1,2): 0,\n",
    "    (1,3): -1,\n",
    "    (2,0): 0,\n",
    "    (2,1): 0,\n",
    "    (2,2): 0,\n",
    "    (2,3): 0\n",
    "}\n",
    "\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "\n",
    "#to reproduce lazy programmer\n",
    "agent.action_probs = {\n",
    "    (0,0): [1., 0.],\n",
    "    (0,1): [0., 1.],\n",
    "    (0,2): [0., 1., 0.],\n",
    "    (1,0): [0., 1.],\n",
    "    (1,2): [0., 1., 0.],\n",
    "    (2,0): [1.0, 0.],\n",
    "    (2,1): [0., 1.],\n",
    "    (2,2): [0., 0., 1.],\n",
    "    (2,3): [1., 0.]\n",
    "}\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n"
     ]
    }
   ],
   "source": [
    "#function for randomly exploring actions during episode, but it should be either exploring starts or this one\n",
    "def random_action(state, action, eps = 0.10):\n",
    "    if random.uniform(0,1) < eps:\n",
    "        return random.choice([a for a in agent.possible_actions[state] if a!= action])\n",
    "    else:\n",
    "        return action\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "#learning rate\n",
    "alpha = 0.10\n",
    "\n",
    "#initialize state values\n",
    "V = {}\n",
    "V = V.fromkeys(rewards.keys(), 0)\n",
    "\n",
    "for it in range(1000):\n",
    "    #start exploration method to reach all states\n",
    "    state = random.choice(list(agent.possible_actions.keys()))\n",
    "    #state = (2,0)\n",
    "    env.set_state(state)\n",
    "    \n",
    "    #collect states and rewards during episode\n",
    "    states_and_rewards = [(state, 0)]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.policy(state)\n",
    "        action = random_action(state, action, eps = 0)\n",
    "        reward, state, done = env.step(action)\n",
    "        states_and_rewards.append((state, reward))\n",
    "\n",
    "    for t in range(len(states_and_rewards) - 1):\n",
    "        state, _ = states_and_rewards[t]\n",
    "        next_state, reward = states_and_rewards[t+1]\n",
    "        V[state] = V[state] + alpha * (reward + gamma * V[next_state] - V[state])\n",
    "        \n",
    "print_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "#get the action corresponding to highest Q-value for a given state\n",
    "def max_dict(d):\n",
    "    max_k = None\n",
    "    max_v = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_v:\n",
    "            max_k = k\n",
    "            max_v = v\n",
    "    return max_k, max_v\n",
    "\n",
    "#randomize the action for being epsilon-greedy\n",
    "def random_action(action, state, eps = 0.20):\n",
    "    if random.uniform(0,1) < eps:\n",
    "        action = random.choice(list(Q[state].keys()))\n",
    "        return action\n",
    "    else: \n",
    "        return action\n",
    "\n",
    "#set the rewards of the game\n",
    "constant = -0.1\n",
    "rewards = {\n",
    "    (0,0): constant,\n",
    "    (0,1): constant,\n",
    "    (0,2): constant,\n",
    "    (0,3): 1,\n",
    "    (1,0): constant,\n",
    "    (1,2): constant,\n",
    "    (1,3): -1,\n",
    "    (2,0): constant,\n",
    "    (2,1): constant,\n",
    "    (2,2): constant,\n",
    "    (2,3): constant\n",
    "}\n",
    "\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.90\n",
    "#learning rate\n",
    "alpha = 0.10\n",
    "\n",
    "#set the environment and the agent\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "\n",
    "#initialize Q-values\n",
    "Q = {}\n",
    "Q = Q.fromkeys(rewards.keys())\n",
    "for s in Q.keys():\n",
    "    if s == (0, 3) or s == (1, 3):\n",
    "        Q[s] = dict((a, 0) for a in ('L', 'R', 'U', 'D'))\n",
    "    else:\n",
    "        Q[s] = dict((a, 0.1 * np.random.normal()) for a in agent.possible_actions[s])\n",
    "        \n",
    "t = 1\n",
    "for it in range(10000):\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        t += 1\n",
    "    \n",
    "    state = (2,0)\n",
    "    env.set_state(state)\n",
    "    action = max_dict(Q[state])[0]\n",
    "    action = random_action(action, state, eps = 0.5 / t)\n",
    "\n",
    "    #collect states and rewards during episode and behave epsilon-greedy\n",
    "    done = False\n",
    "    while not done:\n",
    "        reward, next_state, done = env.step(action)\n",
    "        next_action = max_dict(Q[next_state])[0]\n",
    "        next_action = random_action(next_action, next_state, eps = 0.5 / t)\n",
    "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "#set action probabilities for the agent according to optimal policy\n",
    "for s in agent.action_probs.keys():\n",
    "    agent.action_probs[s] = [0.0] * len(agent.action_probs[s])\n",
    "    agent.action_probs[s][agent.possible_actions[s].index(max_dict(Q[s])[0])] = 1.0\n",
    "\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "The only difference to the SARSA algorithm here is that we choose the maximum Q-value of the next state to learn about the current Q-value, even though this action might not be taken in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "#get the action corresponding to highest Q-value for a given state\n",
    "def max_dict(d):\n",
    "    max_k = None\n",
    "    max_v = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_v:\n",
    "            max_k = k\n",
    "            max_v = v\n",
    "    return max_k, max_v\n",
    "\n",
    "#randomize the action for being epsilon-greedy\n",
    "def random_action(action, state, eps = 0.20):\n",
    "    if random.uniform(0,1) < eps:\n",
    "        action = random.choice(list(Q[state].keys()))\n",
    "        return action\n",
    "    else: \n",
    "        return action\n",
    "\n",
    "#set the rewards of the game\n",
    "constant = -0.1\n",
    "rewards = {\n",
    "    (0,0): constant,\n",
    "    (0,1): constant,\n",
    "    (0,2): constant,\n",
    "    (0,3): 1,\n",
    "    (1,0): constant,\n",
    "    (1,2): constant,\n",
    "    (1,3): -1,\n",
    "    (2,0): constant,\n",
    "    (2,1): constant,\n",
    "    (2,2): constant,\n",
    "    (2,3): constant\n",
    "}\n",
    "\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.90\n",
    "#learning rate\n",
    "alpha = 0.10\n",
    "\n",
    "#set the environment and the agent\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "\n",
    "#initialize Q-values\n",
    "Q = {}\n",
    "Q = Q.fromkeys(rewards.keys())\n",
    "for s in Q.keys():\n",
    "    if s == (0, 3) or s == (1, 3):\n",
    "        Q[s] = dict((a, 0) for a in ('L', 'R', 'U', 'D'))\n",
    "    else:\n",
    "        Q[s] = dict((a, 0.1 * np.random.normal()) for a in agent.possible_actions[s])\n",
    "        \n",
    "t = 1\n",
    "for it in range(10000):\n",
    "\n",
    "    if it % 100 == 0:\n",
    "        t += 1\n",
    "    \n",
    "    state = (2,0)\n",
    "    env.set_state(state)\n",
    "    action, _ = max_dict(Q[state])\n",
    "\n",
    "    #collect states and rewards during episode and behave epsilon-greedy\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = random_action(action, state, eps = 0.5 / t)\n",
    "        reward, next_state, done = env.step(action)\n",
    "        next_action, max_q_next_state = max_dict(Q[next_state])\n",
    "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * max_q_next_state - Q[state][action])\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "#set action probabilities for the agent according to optimal policy\n",
    "for s in agent.action_probs.keys():\n",
    "    agent.action_probs[s] = [0.0] * len(agent.action_probs[s])\n",
    "    agent.action_probs[s][agent.possible_actions[s].index(max_dict(Q[s])[0])] = 1.0\n",
    "\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA with linear approximation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.theta = np.random.normal(size = 25) / np.sqrt(25)\n",
    "\n",
    "    def sa2x(self, s, a):\n",
    "        return np.array([\n",
    "          s[0] - 1              if a == 'U' else 0,\n",
    "          s[1] - 1.5            if a == 'U' else 0,\n",
    "          (s[0]*s[1] - 3)/3     if a == 'U' else 0,\n",
    "          (s[0]*s[0] - 2)/2     if a == 'U' else 0,\n",
    "          (s[1]*s[1] - 4.5)/4.5 if a == 'U' else 0,\n",
    "          1                     if a == 'U' else 0,\n",
    "          s[0] - 1              if a == 'D' else 0,\n",
    "          s[1] - 1.5            if a == 'D' else 0,\n",
    "          (s[0]*s[1] - 3)/3     if a == 'D' else 0,\n",
    "          (s[0]*s[0] - 2)/2     if a == 'D' else 0,\n",
    "          (s[1]*s[1] - 4.5)/4.5 if a == 'D' else 0,\n",
    "          1                     if a == 'D' else 0,\n",
    "          s[0] - 1              if a == 'L' else 0,\n",
    "          s[1] - 1.5            if a == 'L' else 0,\n",
    "          (s[0]*s[1] - 3)/3     if a == 'L' else 0,\n",
    "          (s[0]*s[0] - 2)/2     if a == 'L' else 0,\n",
    "          (s[1]*s[1] - 4.5)/4.5 if a == 'L' else 0,\n",
    "          1                     if a == 'L' else 0,\n",
    "          s[0] - 1              if a == 'R' else 0,\n",
    "          s[1] - 1.5            if a == 'R' else 0,\n",
    "          (s[0]*s[1] - 3)/3     if a == 'R' else 0,\n",
    "          (s[0]*s[0] - 2)/2     if a == 'R' else 0,\n",
    "          (s[1]*s[1] - 4.5)/4.5 if a == 'R' else 0,\n",
    "          1                     if a == 'R' else 0,\n",
    "          1\n",
    "        ])\n",
    "    \n",
    "    def predict(self, state, action):\n",
    "        x = self.sa2x(state, action)\n",
    "        return self.theta.dot(x)\n",
    "    \n",
    "    def grad(self, state, action):\n",
    "        return self.sa2x(state, action)\n",
    "    \n",
    "    def getQs(self, state):\n",
    "        q_vals = {}\n",
    "        for a in Q[state].keys():\n",
    "            q_vals[a] = self.predict(state, a)\n",
    "        return q_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  U  |  U  |\n"
     ]
    }
   ],
   "source": [
    "#get the action corresponding to highest Q-value for a given state\n",
    "def max_dict(d):\n",
    "    max_k = None\n",
    "    max_v = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_v:\n",
    "            max_k = k\n",
    "            max_v = v\n",
    "    return max_k, max_v\n",
    "\n",
    "#randomize the action for being epsilon-greedy\n",
    "def random_action(action, state, eps = 0.20):\n",
    "    if random.uniform(0,1) < eps:\n",
    "        action = random.choice(list(Q[state].keys()))\n",
    "        return action\n",
    "    else: \n",
    "        return action\n",
    "\n",
    "#set the rewards of the game\n",
    "constant = -0.1\n",
    "rewards = {\n",
    "    (0,0): constant,\n",
    "    (0,1): constant,\n",
    "    (0,2): constant,\n",
    "    (0,3): 1,\n",
    "    (1,0): constant,\n",
    "    (1,2): constant,\n",
    "    (1,3): -1,\n",
    "    (2,0): constant,\n",
    "    (2,1): constant,\n",
    "    (2,2): constant,\n",
    "    (2,3): constant\n",
    "}\n",
    "\n",
    "#initialize Q-values\n",
    "Q = {}\n",
    "Q = Q.fromkeys(rewards.keys())\n",
    "for s in Q.keys():\n",
    "    if s == (0, 3) or s == (1, 3):\n",
    "        Q[s] = dict((a, 0) for a in ('L', 'R', 'U', 'D'))\n",
    "    else:\n",
    "        Q[s] = dict((a, 0) for a in agent.possible_actions[s])\n",
    "\n",
    "\n",
    "#discount factor\n",
    "gamma = 0.90\n",
    "#learning rate\n",
    "alpha = 0.10\n",
    "\n",
    "#set the environment and the agent\n",
    "env = Env(rewards)\n",
    "agent = Agent()\n",
    "model = Model()\n",
    "\n",
    "t = 1\n",
    "for it in range(20000):\n",
    "    if it % 100 == 0:\n",
    "        t += 1\n",
    "    \n",
    "    state = (2,0)\n",
    "    env.set_state(state)\n",
    "    Qs = model.getQs(state)\n",
    "    action = max_dict(Qs)[0]\n",
    "    action = random_action(action, state, eps = 0.5 / t)\n",
    "\n",
    "    #collect states and rewards during episode and behave epsilon-greedy\n",
    "    done = False\n",
    "    while not done:\n",
    "        reward, next_state, done = env.step(action)\n",
    "        if done:\n",
    "            model.theta += alpha * (reward - model.predict(state, action)) * model.grad(state, action)\n",
    "        else:\n",
    "            Qns = model.getQs(next_state)\n",
    "            next_action = max_dict(Qns)[0]\n",
    "            next_action = random_action(next_action, next_state, eps = 0.5 / t)\n",
    "            model.theta += alpha * (reward + gamma * model.predict(next_state, next_action) - model.predict(state, action)) * model.grad(state, action)\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "\n",
    "        \n",
    "for s in Q.keys():\n",
    "    for a in Q[s].keys():\n",
    "        Q[s][a] = model.predict(s, a)\n",
    "        \n",
    "#set action probabilities for the agent according to optimal policy\n",
    "for s in agent.action_probs.keys():\n",
    "    agent.action_probs[s] = [0.0] * len(agent.action_probs[s])\n",
    "    agent.action_probs[s][agent.possible_actions[s].index(max_dict(Q[s])[0])] = 1.0\n",
    "\n",
    "print_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
